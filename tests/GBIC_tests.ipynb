{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import copy\n",
    "import os\n",
    "import json\n",
    "from hbic import Hbic\n",
    "from hbic.utils import metrics \n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from biclustlib.evaluation.external import clustering_error\n",
    "from biclustlib.models import Bicluster, Biclustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HL0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:08<00:00,  5.93it/s]\n",
      "100%|██████████| 100/100 [00:18<00:00,  5.54it/s]\n",
      "100%|██████████| 150/150 [00:54<00:00,  2.77it/s]\n",
      "  7%|▋         | 14/200 [00:02<00:39,  4.77it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 128\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sub_exp \u001b[38;5;129;01min\u001b[39;00m sub_experiments:\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28mprint\u001b[39m(sub_exp)\n\u001b[1;32m--> 128\u001b[0m     res, obj \u001b[38;5;241m=\u001b[39m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msub_exp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    129\u001b[0m     results\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m res\n\u001b[0;32m    130\u001b[0m     objects\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m obj\n",
      "Cell \u001b[1;32mIn[2], line 105\u001b[0m, in \u001b[0;36mrun_experiment\u001b[1;34m(experiment, sub_exp)\u001b[0m\n\u001b[0;32m    103\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    104\u001b[0m hbic \u001b[38;5;241m=\u001b[39m Hbic(reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, min_cols\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, min_rows\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, n_clusters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(ref), column_proportion\u001b[38;5;241m=\u001b[39mcolumn_proportion, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m--> 105\u001b[0m biclusters \u001b[38;5;241m=\u001b[39m \u001b[43mhbic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m total_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start\n\u001b[0;32m    107\u001b[0m dic \u001b[38;5;241m=\u001b[39m score_biclusters(biclusters, ref, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m\"\u001b[39m, experiment\u001b[38;5;241m=\u001b[39mexperiment, instance\u001b[38;5;241m=\u001b[39mnb_exp, time\u001b[38;5;241m=\u001b[39mtotal_time, name\u001b[38;5;241m=\u001b[39mname, sub_exp\u001b[38;5;241m=\u001b[39msub_exp)\n",
      "File \u001b[1;32mc:\\users\\chauv\\documents\\github\\py-hbic\\hbic\\hbic.py:328\u001b[0m, in \u001b[0;36mHbic.fit_predict\u001b[1;34m(self, data, var_type, n_clusters)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_predict\u001b[39m(\u001b[38;5;28mself\u001b[39m, data, var_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, n_clusters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    318\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;124;03m    Use the Hbic algorithm and returns biclusters\u001b[39;00m\n\u001b[0;32m    320\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;124;03m    first array is a mask on the rows and the second is a mask on the columns\u001b[39;00m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 328\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbiclusters\n",
      "File \u001b[1;32mc:\\users\\chauv\\documents\\github\\py-hbic\\hbic\\hbic.py:291\u001b[0m, in \u001b[0;36mHbic.fit\u001b[1;34m(self, data, var_type, n_clusters)\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28msum\u001b[39m(remaining_columns) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    290\u001b[0m     temp_data \u001b[38;5;241m=\u001b[39m arr_discretized[rows_ids_bic, :]\n\u001b[1;32m--> 291\u001b[0m     val, column \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_find_best_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremaining_columns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    292\u001b[0m     rows \u001b[38;5;241m=\u001b[39m arr_discretized[:, column] \u001b[38;5;241m==\u001b[39m val\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_column(cols_ids_bic, rows_ids_bic, column, rows)\n",
      "File \u001b[1;32mc:\\users\\chauv\\documents\\github\\py-hbic\\hbic\\hbic.py:134\u001b[0m, in \u001b[0;36mHbic._find_best_column\u001b[1;34m(self, arr_discretized, unclustered_columns)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miteration_values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(unclustered_columns\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(unclustered_columns)):\n\u001b[1;32m--> 134\u001b[0m     values, counts \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[43marr_discretized\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munclustered_columns\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    137\u001b[0m     max_index \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(counts)\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miteration_counts[i] \u001b[38;5;241m=\u001b[39m counts[max_index]\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36munique\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\chauv\\.conda\\envs\\py39\\lib\\site-packages\\numpy\\lib\\arraysetops.py:274\u001b[0m, in \u001b[0;36munique\u001b[1;34m(ar, return_index, return_inverse, return_counts, axis, equal_nan)\u001b[0m\n\u001b[0;32m    272\u001b[0m ar \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(ar)\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 274\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43m_unique1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mequal_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mequal_nan\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unpack_tuple(ret)\n\u001b[0;32m    278\u001b[0m \u001b[38;5;66;03m# axis was specified and not None\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\chauv\\.conda\\envs\\py39\\lib\\site-packages\\numpy\\lib\\arraysetops.py:336\u001b[0m, in \u001b[0;36m_unique1d\u001b[1;34m(ar, return_index, return_inverse, return_counts, equal_nan)\u001b[0m\n\u001b[0;32m    334\u001b[0m     aux \u001b[38;5;241m=\u001b[39m ar[perm]\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 336\u001b[0m     \u001b[43mar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    337\u001b[0m     aux \u001b[38;5;241m=\u001b[39m ar\n\u001b[0;32m    338\u001b[0m mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(aux\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mbool_)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def get_experiment_list(root):\n",
    "    experiments = os.listdir(root)\n",
    "    return experiments\n",
    "\n",
    "\n",
    "def hbic2biclustering(bics):\n",
    "    biclusters = []\n",
    "    for x,y in bics:\n",
    "        x.dtype = bool\n",
    "        y.dtype = bool\n",
    "        biclusters.append(Bicluster(x,y))\n",
    "    return Biclustering(biclusters)\n",
    "\n",
    "\n",
    "def score_subexperiment(experiment, sub_exp, objects, cpt):\n",
    "    data = load_experiment(experiment, sub_exp)\n",
    "    errors = []\n",
    "    for nb_exp,exp in tqdm(enumerate(data), desc=\"Exp:\"+str(experiment), disable=True):\n",
    "        name, arr, ref = exp\n",
    "        h = objects[cpt]\n",
    "        assert h.biclusters[0][0].shape == ref[0][0].shape and h.biclusters[0][1].shape == ref[0][1].shape\n",
    "        ref_biclustering = hbic2biclustering(ref)\n",
    "        identified_bics = hbic2biclustering(h.biclusters)\n",
    "        error = clustering_error(identified_bics, ref_biclustering,ref[0][0].shape[0], ref[0][1].shape[0])\n",
    "        errors.append(error)\n",
    "        cpt+=1\n",
    "        for reduction in [\"distance\",\"pareto\"]:\n",
    "            hbic2 = copy.deepcopy(h)\n",
    "            hbic2.reduction = reduction\n",
    "            hbic2.reduce(n_clusters = len(ref))\n",
    "            identified_bics = hbic2biclustering(hbic2.biclusters)\n",
    "            error = clustering_error(identified_bics, ref_biclustering, ref[0][0].shape[0], ref[0][1].shape[0])\n",
    "            errors.append(error)\n",
    "  \n",
    "    return errors, cpt\n",
    "\n",
    "\n",
    "def index_to_mask(index, shape):\n",
    "    mask = np.zeros(shape, dtype = bool)\n",
    "    mask[index] = 1\n",
    "    return mask\n",
    "\n",
    "def preprocess_ref_json(refs, arr_shape):\n",
    "    keys = set(refs.keys())\n",
    "    bic_keys = keys.intersection({\"biclusters\", \"SymbolicBiclusters\", \"NumericBiclusters\"})\n",
    "    X, Y = [], []\n",
    "    for key in bic_keys:\n",
    "        for bic in refs[key]:\n",
    "            x = refs[key][bic][\"X\"]\n",
    "            y = refs[key][bic][\"Y\"]\n",
    "            X.append(refs[key][bic][\"X\"])\n",
    "            Y.append(refs[key][bic][\"Y\"])\n",
    "    refs = [(index_to_mask(x, arr_shape[0]), index_to_mask(y, arr_shape[1])) for x, y in zip(X, Y)]\n",
    "    return refs\n",
    "\n",
    "def load_experiment(experiment, sub_exp):\n",
    "    path = os.path.join(root, experiment, sub_exp)\n",
    "    files = os.listdir(path)\n",
    "    experiment_arrays = []\n",
    "    experiment_biclusters = []\n",
    "    names = []\n",
    "    for file in files:\n",
    "        if file.endswith('.tsv'):\n",
    "            arr = pd.read_csv(os.path.join(path, file), sep = '\\t')\n",
    "            arr = arr.drop(arr.columns[0], axis=1).to_numpy()\n",
    "            experiment_arrays.append(arr)\n",
    "            names.append(file)\n",
    "        if file.endswith('.json'):\n",
    "            with open(os.path.join(path, file)) as f:\n",
    "                experiment_biclusters.append(json.load(f))\n",
    "    experiment_arrays = [(name, arr, preprocess_ref_json(ref, arr.shape)) for name, arr, ref in zip(names, experiment_arrays, experiment_biclusters)]\n",
    "    return experiment_arrays\n",
    "\n",
    "def score_biclusters(biclusters, ref_bic,reduction, experiment, instance, time,name, sub_exp, column_proportion):\n",
    "    ref_biclustering = hbic2biclustering(ref_bic)\n",
    "    identified_bics = hbic2biclustering(biclusters)\n",
    "\n",
    "    dic = {}\n",
    "    dic[\"rel\"] = metrics.prelic_relevance(biclusters, ref_bic)\n",
    "    dic[\"nb_bics\"] = len(biclusters)\n",
    "    dic[\"rec\"] = metrics.prelic_recovery(biclusters, ref_bic)\n",
    "    dic[\"l_and_w\"] = metrics.l_and_w(biclusters,ref_bic)\n",
    "    dic[\"ayadi\"] = metrics.ayadi(biclusters, ref_bic)\n",
    "    dic[\"bic error\"] = clustering_error(identified_bics, ref_biclustering,ref_bic[0][0].shape[0], ref_bic[0][1].shape[0])\n",
    "    dic[\"reduction\"] = reduction\n",
    "    dic[\"experiment\"] = experiment\n",
    "    dic[\"instance\"] = instance\n",
    "    dic[\"time\"] = time\n",
    "    dic[\"experiment\"] = name\n",
    "    dic[\"sub_experiment\"] = sub_exp\n",
    "    dic[\"column_proportion\"] = column_proportion\n",
    "    return dic\n",
    "\n",
    "root = \"datasets/GBIC/\"\n",
    "\n",
    "\n",
    "def run_experiment(experiment, sub_exp):\n",
    "    data = load_experiment(experiment, sub_exp)\n",
    "    results = []\n",
    "    objects = []\n",
    "    for nb_exp,exp in tqdm(enumerate(data), desc=\"Exp:\"+str(experiment), disable=True):\n",
    "        for column_proportion in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "            name, arr, ref = exp\n",
    "            start = time.time()\n",
    "            hbic = Hbic(reduction=None, min_cols=5, min_rows=5, verbose=True, n_clusters = len(ref), column_proportion=column_proportion, random_state=42)\n",
    "            biclusters = hbic.fit_predict(arr)\n",
    "            total_time = time.time() - start\n",
    "            dic = score_biclusters(biclusters, ref, reduction=\"None\", experiment=experiment, instance=nb_exp, time=total_time, name=name, sub_exp=sub_exp, column_proportion=column_proportion)\n",
    "            results.append(dic)   \n",
    "            objects.append(hbic)\n",
    "            for reduction in [\"distance\",\"pareto\"]:\n",
    "                hbic2 = copy.deepcopy(hbic)\n",
    "                start = time.time()\n",
    "                hbic2.reduction = reduction\n",
    "                hbic2.reduce(n_clusters = len(ref))\n",
    "                biclusters = hbic2.biclusters\n",
    "                dic = score_biclusters(biclusters, ref, reduction=reduction, experiment=experiment, instance=nb_exp, time=time.time() - start + total_time, name = name, sub_exp=sub_exp, column_proportion=column_proportion)\n",
    "                results.append(dic)   \n",
    "    return results, objects\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    experiments = get_experiment_list(root)\n",
    "    results = []\n",
    "    objects = []\n",
    "    for experiment in experiments:\n",
    "        sub_experiments = get_experiment_list(os.path.join(root,experiment))\n",
    "        for sub_exp in sub_experiments:\n",
    "            print(sub_exp)\n",
    "            res, obj = run_experiment(experiment, sub_exp)\n",
    "            results+= res\n",
    "            objects+= obj\n",
    "pd.DataFrame(results).to_csv(\"GBIC results with n_clusters .csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
